{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7afb463b-cf62-4951-925e-397f44604143",
   "metadata": {},
   "source": [
    "# Package introductory notebook : implementing your own PDE problem\n",
    "\n",
    "## Context\n",
    "\n",
    "\n",
    "\n",
    "The **jinns** package is able to solve partial differential equation (PDE) via a machine learning approach. Consider the general statement of a PDE\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\tag{PDE}\n",
    "& \\mathcal{N}_\\eta[u](t, x) = 0, \\quad \\forall  t, x \\in I\\times \\Omega, & \\textrm{(Dynamic)}\\\\\n",
    "& u(0, \\cdot) = u_0(x), \\quad \\forall x \\in \\omega & \\textrm{(Initial condition)} \\\\\n",
    "& \\mathcal{B}[u](dx) = f(dx), \\quad \\forall dx \\in \\partial \\Omega & \\textrm{(Boundary condition)}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "with $\\mathcal{N}_\\eta$ a differential operator involving partial derivatives w.r.t. $t$ and $x$, and $\\mathcal{B}$ is an operator that acts on $\\partial \\Omega$ the border of $\\Omega$. The PDE is said to be stationnary (in time), if $u$ does not change with $t$.\n",
    "\n",
    "\n",
    "The jinns package allows to tackle two different types of problems:\n",
    "\n",
    " 1. **Forward problem :** for a given set of equation parameters $\\eta$, find a parametric function $u_\\hat{\\theta}$ which is a good approximation of the solution $u$ in some sense made precise below.\n",
    " 2. **Inverse problem :** for a given set of observation of the dynamic $\\mathcal{D} = \\{ u(t_i, x_i))\\}_{i=1}^{n_{obs}}$, find the set of equation parameters $\\eta$ that best fits the data.\n",
    "\n",
    "In forward problems, our goal is to learn a parametric function $u_\\theta$ which approximates a solution of (PDE). The physics-informed neural network (PINNS) litterature proposed to use neural networks with weights and bias $\\theta$ to find the best candidate minimizing the loss\n",
    "$$\n",
    "\\hat{u} = u_\\hat{\\theta}\\quad \\textrm{with:} \\quad \\hat{\\theta} \\in \\arg \\min_{\\theta} \\left\\{ L(\\theta) = \\Vert  \\mathcal{N}_\\eta[u_\\theta] \\Vert^2_{dyn} + w_init \\Vert u_{\\theta}(\\cdot, 0) - u_0 \\Vert^2_{init} + w_b \\Vert \\mathcal{B}[u_{\\theta}] - f \\Vert^2_{border} \\right\\},\n",
    "$$\n",
    "where the $(w_b, w_{init})$ are loss weights allowing to calibrate between the different terms. Here, the notation $\\Vert \\cdot \\Vert^2$ corresponds to MSE computed on a discretization of the time interval $I$, the space $\\Omega$ and its border $\\partial \\Omega$. The optimization program may then be solved by (stochastic) gradient descent, using automatic differentiation of $u(\\theta, t, x)$ w.r.t $t$, $x$ and $\\theta$.\n",
    "\n",
    "For inverse problem, we define a function $u_{\\theta, \\nu}$ and wish to find an estimate of the equation parameters $\\hat{\\nu}$ so that \n",
    "$$\n",
    "(\\hat{\\theta}, \\hat{\\nu})  \\in \\arg \\min_{\\theta, \\nu}  \\left\\{ L(\\theta, \\nu) + w_{obs} \\Vert u_{\\theta, \\nu} - u \\Vert_{\\mathcal{D}}^2 \\right\\},\n",
    "$$\n",
    "\n",
    "**Please note:** The problem of meta-model learning a function $u_{\\theta, \\nu}$ giving an approximate solution for any values $\\nu$ is not yet tackled by the package, although we plan to add this feature in the near future. In this case, the function is learnt over a grid of values $\\{\\nu_j\\}$.\n",
    "\n",
    "## Solving forward PDE problems with jinns\n",
    "\n",
    "This tutorial focuses on solving forward problem with **jinns**. In order to define her PDE problem, a user need to define\n",
    "\n",
    " 1. a so-called `DynamicLoss` defining the differential operator $\\mathcal{N}_\\eta[u_\\theta] = 0$.\n",
    " 2. a `DataGenerator` object that handles the generation of the mesh on $I$, $\\Omega$, $\\partial \\Omega$\n",
    "\n",
    "We will work with the toy example of the 1-D Burger equation defined on $I \\times \\Omega = [0, T] \\times [-1, 1]$ as:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{\\partial}{\\partial t} u(t,x) + u(t,x)\\frac{\\partial}{\\partial x}\n",
    "          u(t,x) - \\nu \\frac{\\partial^2}{\\partial x^2} u(t,x) = 0,\\\\\n",
    "          u(0, x) = -\\sin(\\pi x), & \\textrm{ (Initial condition)}\\\\\n",
    "u(t,-1)=u(t,1)=0, & \\textrm{ (Boundary condition)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Although we emphasize that in-depth Notebooks are available for several 2-D stationary and non-stationary PDEs:\n",
    "\n",
    " * Fokker-Planck equation (Ornshtein-Ulhenbeck)\n",
    " * FisherKPP\n",
    " * Burger Equation\n",
    " * Navier-Stokes\n",
    "\n",
    "\n",
    "### The DynamicLoss class\n",
    "The `DynamicLoss` defines the differential operator $\\mathcal{N}_\\eta[u_\\theta] = 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca078b-2631-4224-93d6-ef685646305b",
   "metadata": {},
   "source": [
    "### The stationary case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27595be0-50a2-477e-a6ed-f28a79233ac3",
   "metadata": {},
   "source": [
    "## Solving inverse PDE problems with jinns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32ecbf-9f14-42f3-962f-fcccf09902e8",
   "metadata": {},
   "source": [
    "# Already implemented\n",
    "\n",
    "Some canonical ODE/PDE problems are already implemented in the `loss` submodule \n",
    "\n",
    " * **ODE :** (Generalized) Lotka Volterra\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
